\subsection{Example:  Neural Network Layer}
In order to demonstrate what the language can express, we decided to use a
neural network as an example. Specifically, our example corresponds to a single contraction of a single neural network, with a single layer.

\[nn(y,w,x,b) = L(y,(layer1(w,x,b)))\]
\[nn_c = L \circ (Fst \texttt{`Para`} (Layer1 \circ Snd)) (y, w,x,b)\]
\[nn'_c = L' (Fst \texttt{`Para`} (Layer1 \circ Snd)) (y, w,x,b)\]

\[\text{Layer } = \texttt{map} f (W x + b)\]
\[\text{Loss } = \frac{1}{2} (y-\text{out} )(y-\text{out}) \]

\begin{minted}{haskell}
  (LSec (genVector i) LossFunction)
  `Comp` Zip (map (\(Scalar a) ->
      Scale $ 1 - ((tanh a)^2)) $ map fromIntegral [1..i])
  `Comp` Add
  `Comp` (Para Id (Add `Comp` ((LSec w VecMatProd) `Para` (RSec MatVecProd v))))
\end{minted}

In this example, \tt{v} is a vector of length \tt{i}, and \tt{w} is a square
matrix of the same dimension. The input is a \tt{Pair db (Pair dv dw)} where
both \tt{db} and \tt{dv} are length \tt{i} vectors, and \tt{dw} is a square
matrix like \tt{w}.

The combined \tt{LFun} is appled to this value with the rightmost \tt{LFun}
in the \tt{Comp} chain first. This is a parallel operation, the left part of
which leaves \tt{db} unchanged, and the right part of which adds the result
of applying \tt{(LSec w VecMatProd) `Para` (RSec MatVecProd v)} on the
\tt{(Pair dv dw)} pair. This result is a vector, which is then added to
\tt{db}. Then a Zip of scale functions is applied to this, which simulates
the work of the deltamap. In reality, the value contents of this zip depends
on the values of the other constant values, and is computed at compile time,
so to save time in benchmarking, we have simulated the runtime by a
work-equivalent series of \tt{Scale} operations. Finally, the loss function
is applied to the result.

\begin{minted}{haskell}
open import "lmaplib"

let fun1 = (flip inner_2_1 [2.0f32, 2.0f32])
let fun2 = (inner_2_1 [[2.0f32, 2.0f32], [2.0f32, 2.0f32]])
let fun3 = (para fun2 fun1)
let fun4 = (add_1_1)
let fun5 = (comp fun4 fun3)
let fun6 = (id)
let fun7 = (para fun6 fun5)
let fun8 = (add_1_1)
let fun9 = (comp fun8 fun7)
let fun10 = (uncurry outer_0_0)
let fun11 = (specMap2 fun10 [0.41997434161402614f32,7.065082485316443e-2f32])
let fun12 = (comp fun11 fun9)
let fun13 = (lossFunction_1_1 [2.0f32, 2.0f32])
let fun14 = (comp fun13 fun12)
entry main (arg1: []f32) (arg2: []f32) (arg3: [][]f32) = fun14 (arg1, (arg2, arg3))
\end{minted}

The above is the resulting Futhark program when $i==2$ and we run it through the
regular compiler. Note how the three input arguments are implicity given to the
entrypoint separately, and then recomposed in the application of \tt{fun14}.
Note also the resemblance to three-address code.

\begin{minted}{haskell}
open import "lmaplib"
let fun1 = (uncurry (flip inner_2_1))
let fun2 = (uncurry inner_2_1)
let fun3 = (constPassingPara fun2 fun1)
let fun4 = (ignoreDummyVal add_1_1)
let fun5 = (constPassingComp fun4 fun3)
let fun6 = (ignoreDummyVal id)
let fun7 = (constPassingPara fun6 fun5)
let fun8 = (ignoreDummyVal add_1_1)
let fun9 = (constPassingComp fun8 fun7)
let fun10 = (uncurry outer_0_0)
let fun11 = (constPassingMap2 fun10)
let fun12 = (constPassingComp fun11 fun9)
let fun13 = (uncurry lossFunction_1_1)
let fun14 = (constPassingComp fun13 fun12)
entry main (arg1: []f32) (arg2: []f32) (arg3: [][]f32) = fun14 (([2.0f32, 2.0f32], ([0.41997434161402614f32, 7.065082485316443e-2f32], (f32.nan, (f32.nan, (f32.nan, ([[2.0f32, 2.0f32], [2.0f32, 2.0f32]], [2.0f32, 2.0f32])))))), (arg1, (arg2, arg3)))
\end{minted}

This time, the example has been compiled with the constant extracting compiler.
It doesn't generate more human-readable code, quite the contrary. Note the
composition of the partially applied functions without the constants, and
how these are composed in the entry point. It no longer resembles three-address code.
